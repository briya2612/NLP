{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQ44lTTst_Bz",
        "outputId": "ae73516a-c217-4601-a18c-9fcbd714294d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.64.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk) (2022.6.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n"
          ]
        }
      ],
      "source": [
        "pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('brown')\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('genesis')\n",
        "nltk.download('inaugural')\n",
        "nltk.download('nps_chat')\n",
        "nltk.download('webtext')\n",
        "nltk.download('treebank')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOUPmcuMuOHc",
        "outputId": "f68e3774-5864-42ad-9dd6-a26920e19f68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "[nltk_data] Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]   Package genesis is already up-to-date!\n",
            "[nltk_data] Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]   Package inaugural is already up-to-date!\n",
            "[nltk_data] Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]   Package nps_chat is already up-to-date!\n",
            "[nltk_data] Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]   Package webtext is already up-to-date!\n",
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Package treebank is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ldDYcWaEuUxB",
        "outputId": "5a0bfa4e-753e-41a6-ff09-63dfef6e4f2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'3.7'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import brown\n"
      ],
      "metadata": {
        "id": "XK1cCIQRucXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "brown.words()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mkjLT0Fbu3QN",
        "outputId": "3ca023fb-67b7-46d5-9aa5-fe7d3f95952a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(brown.words())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtc8ayMGvBYt",
        "outputId": "90dc75c2-756e-449b-9c66-410b9e8bbfa7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1161192"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "brown.categories()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fsX7GhLkwcqq",
        "outputId": "8b9c5bca-9f8d-4faa-bb43-9464b7222278"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['adventure',\n",
              " 'belles_lettres',\n",
              " 'editorial',\n",
              " 'fiction',\n",
              " 'government',\n",
              " 'hobbies',\n",
              " 'humor',\n",
              " 'learned',\n",
              " 'lore',\n",
              " 'mystery',\n",
              " 'news',\n",
              " 'religion',\n",
              " 'reviews',\n",
              " 'romance',\n",
              " 'science_fiction']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.book import *"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJiHzWgtwgCd",
        "outputId": "44a25b64-f004-400e-b166-c77198a2343d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Introductory Examples for the NLTK Book ***\n",
            "Loading text1, ..., text9 and sent1, ..., sent9\n",
            "Type the name of the text or sentence to view it.\n",
            "Type: 'texts()' or 'sents()' to list the materials.\n",
            "text1: Moby Dick by Herman Melville 1851\n",
            "text2: Sense and Sensibility by Jane Austen 1811\n",
            "text3: The Book of Genesis\n",
            "text4: Inaugural Address Corpus\n",
            "text5: Chat Corpus\n",
            "text6: Monty Python and the Holy Grail\n",
            "text7: Wall Street Journal\n",
            "text8: Personals Corpus\n",
            "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##Regexp Tokenizer\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "tokenizer=RegexpTokenizer(r'\\w+')\n",
        "text=tokenizer.tokenize('Hello all, this is thr first')\n",
        "text\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GIxxmpWvwpz0",
        "outputId": "07121486-1260-4b55-96bd-006e17bef1ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello', 'all', 'this', 'is', 'thr', 'first']"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "texty=\"god is great! I won a lottery\"\n",
        "print(word_tokenize(texty))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GeNQXj6Ty7to",
        "outputId": "9a89e779-eacf-4e9d-b94a-8557eb7c0ced"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['god', 'is', 'great', '!', 'I', 'won', 'a', 'lottery']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##TweetTokwniser\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "tok=TweetTokenizer()\n",
        "s=\"this is #: :-) :-p <3 <> -> <--\"\n",
        "tok.tokenize(s)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3202LH_zPt6",
        "outputId": "ca6b7eba-3db5-4652-a4aa-da533081309f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['this', 'is', '#', ':', ':-)', ':-p', '<3', '<', '>', '->', '<--']"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tok=TweetTokenizer(strip_handles=True, reduce_len=True)\n",
        "s='@remy: His name :0 :) !!'\n",
        "tok.tokenize(s)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8rBqiVn0qRe",
        "outputId": "d414d128-3e27-42e4-f069-153bd4140c07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[':', 'His', 'name', ':', '0', ':)', '!', '!']"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tok=TweetTokenizer(preserve_case=False)\n",
        "s=\"@jrmy: I'M REALLY HAPPY ;)\"\n",
        "tok.tokenize(s)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WfqwJ4D41JHY",
        "outputId": "264de548-a99e-4db4-df3d-1cf426c65f9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['@jrmy', ':', \"i'm\", 'really', 'happy', ';)']"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "text=\"God is one. I won this\"\n",
        "sent_tokenize(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3yFyOABR1eX1",
        "outputId": "70c787d4-b075-4f5b-ef60-57d8921ccfd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['God is one.', 'I won this']"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "text=re.sub(r\"[^a-zA-Z0-9]\",\" \",text.lower())\n",
        "words=text.split()\n",
        "words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCpoFwGb28uh",
        "outputId": "53ddb8d1-dc54-495c-c694-bb718ed9cf5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['god', 'is', 'one', 'i', 'won', 'this']"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "stopwords.words('english')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qz1r4Fpq3ZqN",
        "outputId": "8a788350-d227-4832-b7e3-7044b2bde253"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i',\n",
              " 'me',\n",
              " 'my',\n",
              " 'myself',\n",
              " 'we',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'you',\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " \"you'll\",\n",
              " \"you'd\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " 'he',\n",
              " 'him',\n",
              " 'his',\n",
              " 'himself',\n",
              " 'she',\n",
              " \"she's\",\n",
              " 'her',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'it',\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'they',\n",
              " 'them',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'themselves',\n",
              " 'what',\n",
              " 'which',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'this',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'these',\n",
              " 'those',\n",
              " 'am',\n",
              " 'is',\n",
              " 'are',\n",
              " 'was',\n",
              " 'were',\n",
              " 'be',\n",
              " 'been',\n",
              " 'being',\n",
              " 'have',\n",
              " 'has',\n",
              " 'had',\n",
              " 'having',\n",
              " 'do',\n",
              " 'does',\n",
              " 'did',\n",
              " 'doing',\n",
              " 'a',\n",
              " 'an',\n",
              " 'the',\n",
              " 'and',\n",
              " 'but',\n",
              " 'if',\n",
              " 'or',\n",
              " 'because',\n",
              " 'as',\n",
              " 'until',\n",
              " 'while',\n",
              " 'of',\n",
              " 'at',\n",
              " 'by',\n",
              " 'for',\n",
              " 'with',\n",
              " 'about',\n",
              " 'against',\n",
              " 'between',\n",
              " 'into',\n",
              " 'through',\n",
              " 'during',\n",
              " 'before',\n",
              " 'after',\n",
              " 'above',\n",
              " 'below',\n",
              " 'to',\n",
              " 'from',\n",
              " 'up',\n",
              " 'down',\n",
              " 'in',\n",
              " 'out',\n",
              " 'on',\n",
              " 'off',\n",
              " 'over',\n",
              " 'under',\n",
              " 'again',\n",
              " 'further',\n",
              " 'then',\n",
              " 'once',\n",
              " 'here',\n",
              " 'there',\n",
              " 'when',\n",
              " 'where',\n",
              " 'why',\n",
              " 'how',\n",
              " 'all',\n",
              " 'any',\n",
              " 'both',\n",
              " 'each',\n",
              " 'few',\n",
              " 'more',\n",
              " 'most',\n",
              " 'other',\n",
              " 'some',\n",
              " 'such',\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'only',\n",
              " 'own',\n",
              " 'same',\n",
              " 'so',\n",
              " 'than',\n",
              " 'too',\n",
              " 'very',\n",
              " 's',\n",
              " 't',\n",
              " 'can',\n",
              " 'will',\n",
              " 'just',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'now',\n",
              " 'd',\n",
              " 'll',\n",
              " 'm',\n",
              " 'o',\n",
              " 're',\n",
              " 've',\n",
              " 'y',\n",
              " 'ain',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'ma',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\"]"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "text=\"Today is a great day. It is even better than yest. And yest was the best day\"\n",
        "stop=set(stopwords.words('english'))\n",
        "from nltk.tokenize import word_tokenize\n",
        "words=word_tokenize(text)\n",
        "arr=[]\n",
        "for w in words:\n",
        "  if w not in stop:\n",
        "    arr.append(w)\n",
        "arr\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i0-Hsubl3nw9",
        "outputId": "35207c6e-7b83-4735-8dd9-2916e0ce4182"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Today',\n",
              " 'great',\n",
              " 'day',\n",
              " '.',\n",
              " 'It',\n",
              " 'even',\n",
              " 'better',\n",
              " 'yest',\n",
              " '.',\n",
              " 'And',\n",
              " 'yest',\n",
              " 'best',\n",
              " 'day']"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import SnowballStemmer\n",
        "porter=PorterStemmer()\n",
        "snowball=SnowballStemmer(language='english')\n",
        "print(\"PorterStemmer\")\n",
        "print(porter.stem('cats'))\n",
        "print(porter.stem(\"trouble\"))\n",
        "print(porter.stem(\"troubling\"))\n",
        "print(porter.stem(\"troubled\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wx_nqDgn4lyx",
        "outputId": "37bd23ef-d2c3-46b0-8469-e3a7d7e48069"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PorterStemmer\n",
            "cat\n",
            "troubl\n",
            "troubl\n",
            "troubl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import LancasterStemmer\n",
        "lancas=LancasterStemmer()\n",
        "print(lancas.stem(\"Universe\"))\n",
        "print(lancas.stem(\"University\"))\n",
        "print(lancas.stem(\"Universal\"))\n",
        "print(lancas.stem(\"Universities\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBKc8xuR-JCz",
        "outputId": "b2fa856f-3575-4b5a-d414-1e824a8a3a71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "univers\n",
            "univers\n",
            "univers\n",
            "univers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"fairly\")\n",
        "print(\"porter\",porter.stem('fairly'))\n",
        "print(\"snowball\",snowball.stem('fairly'))\n",
        "print(\"lancas\",lancas.stem('fairly'))\n",
        "print(\"salty\")\n",
        "print(\"porter\",porter.stem('salty'))\n",
        "print(\"snowball\",snowball.stem('salty'))\n",
        "print(\"lancas\",lancas.stem('salty'))\n",
        "print(\"generically\")\n",
        "print(\"porter\",porter.stem('generically'))\n",
        "print(\"snowball\",snowball.stem('generically'))\n",
        "print(\"lancas\",lancas.stem('generically'))\n",
        "print(\"generous\")\n",
        "print(\"porter\",porter.stem('generous'))\n",
        "print(\"snowball\",snowball.stem('generous'))\n",
        "print(\"lancas\",lancas.stem('generous'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lFaIkQl3_n9Q",
        "outputId": "35dcd731-76e8-4615-c2fd-18caeddf2de6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fairly\n",
            "porter fairli\n",
            "snowball fair\n",
            "lancas fair\n",
            "salty\n",
            "porter salti\n",
            "snowball salti\n",
            "lancas sal\n",
            "generically\n",
            "porter gener\n",
            "snowball generic\n",
            "lancas gen\n",
            "generous\n",
            "porter gener\n",
            "snowball generous\n",
            "lancas gen\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plurals=['caresses','files','dies','mules','denied','died','agreed','owned','humbled','sized','meeting','stating','siezing','itemization','sensational','traditional','reference','colonizer','plotted']\n",
        "single=[porter.stem(plural) for plural in plurals]\n",
        "print(' '.join(single))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kyGYRXjtB3dQ",
        "outputId": "a51aaf78-100a-4186-eb40-484a6e0b90c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "caress file die mule deni die agre own humbl size meet state siez item sensat tradit refer colon plot\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet\n",
        "syn=wordnet.synsets('happy')\n",
        "print(\"Synonym\",syn)\n",
        "print(\"def\",syn[0].definition())\n",
        "print(\"eg\",syn[0].examples())\n",
        "print(\"lemma\",syn[0].lemmas())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "st1wDrZ9EHSA",
        "outputId": "4581cb69-0ffc-463f-cbf0-4987bba4bf84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Synonym [Synset('happy.a.01'), Synset('felicitous.s.02'), Synset('glad.s.02'), Synset('happy.s.04')]\n",
            "def enjoying or showing or marked by joy or pleasure\n",
            "eg ['a happy smile', 'spent many happy days on the beach', 'a happy marriage']\n",
            "lemma [Lemma('happy.a.01.happy')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "syn=wordnet.synsets('bike')\n",
        "print(\"Synonym\",syn)\n",
        "print(\"def\",syn[0].definition())\n",
        "print(\"eg\",syn[0].examples())\n",
        "print(\"lemma\",syn[0].lemmas())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJ2BGbfX6b-5",
        "outputId": "f609b9f1-6f7c-4a5a-853f-ba744cf8e336"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Synonym [Synset('motorcycle.n.01'), Synset('bicycle.n.01'), Synset('bicycle.v.01')]\n",
            "def a motor vehicle with two wheels and a strong frame\n",
            "eg []\n",
            "lemma [Lemma('motorcycle.n.01.motorcycle'), Lemma('motorcycle.n.01.bike')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(wordnet.synsets('dog'))\n",
        "print(wordnet.synsets('dog',pos=wordnet.VERB))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7f4JgUA674_",
        "outputId": "2dc5dcff-4d97-411c-cd6a-a40809b11a84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Synset('dog.n.01'), Synset('frump.n.01'), Synset('dog.n.03'), Synset('cad.n.01'), Synset('frank.n.02'), Synset('pawl.n.01'), Synset('andiron.n.01'), Synset('chase.v.01')]\n",
            "[Synset('chase.v.01')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sorted(wordnet.langs())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xVvcV4L7eJY",
        "outputId": "8c1b278e-7e03-4477-e9af-562291ca794d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['als',\n",
              " 'arb',\n",
              " 'bul',\n",
              " 'cat',\n",
              " 'cmn',\n",
              " 'dan',\n",
              " 'ell',\n",
              " 'eng',\n",
              " 'eus',\n",
              " 'fin',\n",
              " 'fra',\n",
              " 'glg',\n",
              " 'heb',\n",
              " 'hrv',\n",
              " 'ind',\n",
              " 'isl',\n",
              " 'ita',\n",
              " 'ita_iwn',\n",
              " 'jpn',\n",
              " 'lit',\n",
              " 'nld',\n",
              " 'nno',\n",
              " 'nob',\n",
              " 'pol',\n",
              " 'por',\n",
              " 'ron',\n",
              " 'slk',\n",
              " 'slv',\n",
              " 'spa',\n",
              " 'swe',\n",
              " 'tha',\n",
              " 'zsm']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "syn=wordnet.synsets('hello')[0]\n",
        "print(\"Name\",syn.name())\n",
        "print(\"abstract term\",syn.hypernyms())\n",
        "print(\"Specific term\",syn.hypernyms()[0].hyponyms())\n",
        "syn.root_hypernyms()\n",
        "print(\"root hypernyms\",syn.root_hypernyms())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "niXfsNTL751t",
        "outputId": "a28e6ae5-3924-46d3-c475-a8da6e53d838"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name hello.n.01\n",
            "abstract term [Synset('greeting.n.01')]\n",
            "Specific term [Synset('calling_card.n.02'), Synset('good_afternoon.n.01'), Synset('good_morning.n.01'), Synset('hail.n.03'), Synset('hello.n.01'), Synset('pax.n.01'), Synset('reception.n.01'), Synset('regard.n.03'), Synset('salute.n.02'), Synset('salute.n.03'), Synset('welcome.n.02'), Synset('well-wishing.n.01')]\n",
            "root hypernyms [Synset('entity.n.01')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dog=wordnet.synset('dog.n.01')\n",
        "print('hypernyms',dog.hypernyms())\n",
        "print(\"hyponyms\",dog.hyponyms())\n",
        "print(\"member holonyms\",dog.member_holonyms())\n",
        "print(\"root hypernyms\",dog.root_hypernyms())\n",
        "print(\"lowest common hypernyms\",dog.lowest_common_hypernyms(wordnet.synset('cat.n.01')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YnFSpw6tBb2t",
        "outputId": "c7151cca-907a-4902-cdf9-3541cc500075"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hypernyms [Synset('canine.n.02'), Synset('domestic_animal.n.01')]\n",
            "hyponyms [Synset('basenji.n.01'), Synset('corgi.n.01'), Synset('cur.n.01'), Synset('dalmatian.n.02'), Synset('great_pyrenees.n.01'), Synset('griffon.n.02'), Synset('hunting_dog.n.01'), Synset('lapdog.n.01'), Synset('leonberg.n.01'), Synset('mexican_hairless.n.01'), Synset('newfoundland.n.01'), Synset('pooch.n.01'), Synset('poodle.n.01'), Synset('pug.n.01'), Synset('puppy.n.01'), Synset('spitz.n.01'), Synset('toy_dog.n.01'), Synset('working_dog.n.01')]\n",
            "member holonyms [Synset('canis.n.01'), Synset('pack.n.06')]\n",
            "root hypernyms [Synset('entity.n.01')]\n",
            "lowest common hypernyms [Synset('carnivore.n.01')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hmjLl8zVCfH1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}